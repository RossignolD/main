**Prerequisites**

- posets (lattices)

# The lattice space of $n$-gram grammars

The general take-home message of this chapter has been that a surprising number of seemingly unrelated language universals can be stated as monotonicity properties over ordered sets.
This includes both substantive universals ($^*$ABA, PCC) and formal universals (adjunct island constraint).
But monotonicity is even more general.
We conclude our journey with a brief look at how language acquisition might be informed by monotonicity, too.


## SL and learning

Alright, time to rack your brain: do you still remember what a negative strictly $n$-local (SL-$n$) grammar is?
It's a finite set of $n$-grams such that a string is well-formed iff it does not contain any of the $n$-grams as a substring (after adding $n-1$ instances of \$ on each side of the string).
A positive strictly $n$-local grammar is the counterpart where a string may only consists of $n$-grams that are allowed by the grammar.
Every negative SL grammar can be translated into a positive one, and the other way round.

\begin{exercise}
As a reminder for yourself, write down an SL grammar for an attested linguistic phenomenon.
A simple option is word-final devoicing or intervocalic voicing, but feel free to mix it up a bit.
\end{exercise}

Suppose we have a phenomenon that is SL.
Then of course we can write an SL grammar for it.
But how exactly do we come up with the grammar?
And more generally, how can this phenomenon be acquired?
Is there some algorithm that a child might use to identify the correct grammar for an SL process?

Depending on your background, you might wonder why anybody would care how children find the right grammar for their language.
As long as they actually do it and learn the grammar, everything's fine and we don't need to worry about this.
But learning is in fact the most important question of linguistics because it is, to put it bluntly, a freaking miracle.
Languages are very complex systems, yet children master them effortlessly without explicit instruction and with very little data.
Think back to the unit on negative polarity items and *ever*.
Unless you had prior linguistic training, you were completely unaware of the laws that govern the use of *ever*, yet you've obeyed them your whole life.
If we look at the sentences that children hear around them while growing up, *ever* isn't all that common.
There isn't millions of sentences with *ever* that the child could study in order to figure out how those negative polarity items work.
The child takes the astonishingly little data it gets and still solves the problem almost effortlessly.
We have no idea how children do this --- it truly is a miracle.

Learning is also of supreme importance for practical applications, e.g. voice recognition or machine translation. 
Computational linguists do not design those systems by hand.
That would be an impossible task.
It is sometimes done for very small and specialized domains like reading out numbers ($1729$ as a year is *seventeen hundred twenty nine*, but as a phone number it is *one seven two nine*).
But in general grammars are learned automatically from an enormous collection of data, which is also called a **corpus**.
The process of inducing a grammar or model from a corpus is called **machine learning** or **grammatical inference**.
Needless to say, grammatical inference is impossible without a learning algorithm.

So whether you care about the mysteries of the human mind or just want to see better closed captioning on Youtube, learning is extremely important.
It is also a huge and very demanding research area.
The learning algorithm for SL is very simple but also pretty clever.
That makes it a good sneak peek of what the field of grammatical inference is like.


## Learning by memorization

Let's conduct a thought experiment.
Suppose that you have to learn the phonotactics of a new language (recall that phonotactics is the collection of rules and laws that govern the distribution of sounds).
The only think you are told about the language is is that its phonotactics are at most strictly $3$-local.
The rest you have to figure out from the data, which is a never-ending stream of well-formed examples, one after the other.
How could you do this?

Well, I don't know how you're going to approach it, but here's my strategy, which is guaranteed to work. 
First, since the phonotactics are at most strictly $3$-local, I will assume that they are exactly strictly $3$-local, no more, no less.
I am allowed to do that because every a strictly $n$-local language is also strictly $n+1$-local, so even if the language is in fact strictly $1$-local or strictly $2$-local, it doesn't hurt me to assume that it is strictly $3$-local.
Next, I will use the fact that every strictly $3$-local language can be described by a positive SL-$3$ grammar, which is a finite set of allowed trigrams.
This means that I only have to figure out what this finite set of allowed trigrams is.
And that is easy: I look at every example string and memorize which trigrams occur in it.
Since there can be only finitely many allowed trigrams, eventually I will have seen all of them, and at that point I have learned the phonotactics of the language.

\begin{example}
Suppose that we have to learn the (infinite) SL-$2$ language of all strings that start with either $a$ or $b$ and where $a$ and $b$ have to alternate.
This language contains the following strings, starting from the shortest:

<ol>
<li>$\String{a}$</li>
<li>$\String{b}$</li>
<li>$\String{ab}$</li>
<li>$\String{ba}$</li>
<li>$\String{aba}$</li>
<li>$\String{bab}$</li>
<li>$\String{abab}$</li>
<li>$\String{baba}$</li>
<li>$\String{ababab}$</li>
<li>$\String{bababa}$</li>
<li>and so on</li>
</ol>

Just so we're on the same page, here's the positive SL-$2$ grammar that generates this language.

<ol>
<li>$\String{\$a}$</li>
<li>$\String{\$b}$</li>
<li>$\String{ab}$</li>
<li>$\String{ba}$</li>
<li>$\String{a\$}$</li>
<li>$\String{b\$}$</li>
<li>and so on</li>
</ol>

That's the grammar we want to learn from the examples.

Now assume that the first example we get to look at is $\String{ab}$.
We extract all the bigrams, giving you a short list of allowed bigrams.

<ol>
<li>$\String{\$a}$</li>
<li>$\String{ab}$</li>
<li>$\String{b\$}$</li>
</ol>

This grammar can only generate one string, namely the example $\String{ab}$ that we derived it from.
That's not particularly interesting, we've looked at one well-formed example and have constructed a grammar that only allows for this one example.

Well, let's look at the next example.
Let's assume that it is $\String{baba}$.
Once again we extract the bigrams and add them to what we have so far, giving us the list below.

<ol>
<li>$\String{\$a}$</li>
<li>$\String{\$b}$</li>
<li>$\String{ab}$</li>
<li>$\String{ba}$</li>
<li>$\String{a\$}$</li>
<li>$\String{b\$}$</li>
</ol>

Lo and behold, it's the grammar for the target language.
Two examples were enough to learn the whole infinite set of well-formed strings!
\end{example}

\begin{exercise}
Suppose that you have the SL-$3$ language $(\String{abc})^+$, which contains $\String{abc}$, $\String{abcabc}$, $\String{abcabcabc}$, and so on.
Write down the positive trigram grammar for this language.
Then write down a single example from which one can immediately infer the whole grammar.
\end{exercise}

As you can see, learning SL-$n$ languages is easy as long as there is a fixed upper bound on the value of $n$.
In that case, we simply memorize all $n$-grams we see in the examples, and eventually we will have seen all well-formed $n$-grams.
At that point, we have learned the language.

But hold on a second, how many examples do we actually need to look at?
In the examples above one or two examples are enough, but those are toy examples.
Maybe real languages require us to see trillions of examples, in which case our learning strategy would be completely useless in practice.
In order for a learning algorithm to perform well, it should work with as little data as possible.
So let's do some calculations.


## Learning efficiency and the grammar lattice

At first glance, you might think that the math looks pretty bad for learning SL languages.
Let's consider the problem of learning a specific SL-$5$ language $L$ over an alphabet of about 50 symbols.
This is realistic for the phonotactics of natural languages, were at least some processes are SL-$4$ or SL-$5$, and many languages have 50 to 100 different sounds.
With 50 symbols, there are $50^5$ different $5$-grams.
That's $312,500,000$ different $n$-grams.
Okay, that's big.
But it absolutely pales in comparison to the number of possible $5$-grams grammars, which is $2^{312,500,000}$.
This number is so large we would need a new word to describe it adequately.
It is greater that the number of seconds since the Big Bang.
If you were given $2^{312,500,000}$ sheets of paper and used them to build a paper tower from New York City to the sun, you would still be left with 99,99999\ldots\% of the paper afterwards.
Whatever you think is a large number, it isn't even a drop in the ocean compared to $2^{312,500,000}$.
And among those $2^{312,500,000}$ grammars, we have to find one for $L$.
Talk about looking for a needle in a haystack.

But the learning procedure we sketched above is actually much smarter than this.
It never looks at all possible grammars, it just keeps track of $n$-grams, and that greatly cuts down the space of options.
To see why, we will have to look at the structure of the space of possible grammars.
As it turns out, this is a very peculiar structure for $n$-gram grammars.

Given a fixed value for $n$, we can look at the set of all possible positive $n$-gram grammars.
Let's call this set $\mathcal{G}$.
As we just saw, $\mathcal{G}$ can be huge, but it will always be finite.
Since each $n$-gram grammar is itself a set, we can order the set of $n$-gram grammars by the subset relation.
This gives us a poset, which we will refer to by $\mathbb{G}$.

\begin{example}
Suppose $n = 2$ and our alphabet $\Sigma$ contains only $a$.
We add the edge marker to $\Sigma$ to give us the extended alphabet $\Sigma_\$$.
There are 16 distinct bigram grammars over $\Sigma_\$$.

<ol>
<li>$\emptyset$</li>
<li>$\setof{\$\$}$</li>
<li>$\setof{\$a}$</li>
<li>$\setof{a\$}$</li>
<li>$\setof{aa}$</li>
<li>$\setof{\$\$, \$a}$</li>
<li>$\setof{\$\$, a\$}$</li>
<li>$\setof{\$\$, aa}$</li>
<li>$\setof{\$a, a\$}$</li>
<li>$\setof{\$a, aa}$</li>
<li>$\setof{a\$, aa}$</li>
<li>$\setof{\$\$, \$a, a\$}$</li>
<li>$\setof{\$\$, \$a, aa}$</li>
<li>$\setof{\$\$, a\$, aa}$</li>
<li>$\setof{\$a, a\$, aa}$</li>
<li>$\setof{\$\$, \$a, a\$, aa}$</li>
</ol>

Ordering these sets by the subset relation $\subseteq$ yields the following structure.

\input_mid{./lattice_bigrams.tikz}
\end{example}

The structure in the above example is rather special.
For any two grammars $G_1$ and $G_2$, there is a unique smallest grammar $G_\vee$ such that $G_1, G_2 \subseteq G_\vee$.
Similarly, there's also a unique largest grammar $G_\wedge$ such that $G_1, G_2 \supseteq G_\wedge$.
In other words, $\mathcal{G}$ is both a meet semilattice and a join semilattice.
This makes $\mathbb{G}$ a **lattice**.

In fact, $\mathbb{G}$ is a **powerset lattice**.
The powerset of a set $S$ is $\wp(S) \is \setof{ X \mid X \subseteq S}$.
In other words, the powerset of $S$ consists of all subsets of $S$, including the empty set and $S$ itself.
The grammars we listed above are all subsets of $\Sigma_\$^2$.
Therefore, $\mathbb{G}$ is $\wp(\Sigma_\$^2)$, sorted by $\subseteq$.

Alright, that paragraph might have taken you three or four rereads to get it, let's see why this is worth the effort you put into it.

## The lattice of SL languages

Each grammar $G$ of $\mathcal{G}$ generates a unique string language $L(G)$.
This language is just the set of strings that are well-formed with respect to $G$.
Let us call the set of all these languages $\mathcal{L}$.
More formally, $\mathcal{L} \is \setof{ L(G) \mid G \in \mathcal{G} }$.
Just like $\mathcal{G}$, we can order $\mathcal{L}$ by $\subseteq$ to obtain a lattice $\mathbb{L} \is \tuple{\mathcal{L}, \subseteq}$.

\begin{example}
Recall that all grammars in $\mathcal{G}$ are assumed to be positive.
Then the languages generated by these grammars are as follows:

<ol>
<li>$L(\emptyset) = \emptyset$</li>
<li>$L(\setof{\$\$} = \setof{\emptystring}$</li>
<li>$L(\setof{\$a} = \emptyset$</li>
<li>$L(\setof{a\$} = \emptyset$</li>
<li>$L(\setof{aa} = \emptyset$</li>
<li>$L(\setof{\$\$, \$a} = \setof{\emptystring}$</li>
<li>$L(\setof{\$\$, a\$} = \setof{\emptystring}$</li>
<li>$L(\setof{\$\$, aa} = \setof{\emptystring}$</li>
<li>$L(\setof{\$a, a\$} = \setof{a}$</li>
<li>$L(\setof{\$a, aa} = \emptyset$</li>
<li>$L(\setof{a\$, aa} = \emptyset$</li>
<li>$L(\setof{\$\$, \$a, a\$} = \setof{\emptystring, a}$</li>
<li>$L(\setof{\$\$, \$a, aa} = \setof{\emptystring}$</li>
<li>$L(\setof{\$\$, a\$, aa} = \setof{\emptystring}$</li>
<li>$L(\setof{\$a, a\$, aa} = \setof{a, aa, aaa, \ldots} = a^+$ (1 or more $a$s)</li>
<li>$L(\setof{\$\$, \$a, a\$, aa} = \setof{\emptystring, a, aa, aaa, \ldots} = a^*$ (0 or more $a$s)</li>
</ol>
\end{example}

Let us put the lattices from the two examples next to each other.
We also add frames around nodes to indicate which grammar generates which language.

\input_large{./lattice_bigrams_match.tikz}

Notice anything special?
Whenever two grammars stand in the subset relation, their languages do, too.
That is to say, $G \subseteq G'$ implies $L(G) \subseteq L(G')$.
Once again we've stumbled across monotonicity.
The relation between grammars and their languages is order-preserving: move to a larger positive grammar, and you either get the same string language or a larger one.
This monotonicity connection allows for a very natural learning algorithm for SL languages.

\begin{exercise}
Suppose that all members of $\mathcal{G}$ are negative grammars.
Draw the corresponding lattice of languages, and connect each grammar to its language with a colored line.
\end{exercise}

\begin{exercise}
What is the relation between negative grammars and their languages?
Is it still monotonic?
If so, is it the same kind of monotonicity?
\end{exercise}

## Learning SL languages

Suppose that we have to learn some SL language $L$ from a sample of well-formed strings.
All we know about $L$ is that it is, say, SL-3, and what its alphabet $\Sigma$ looks like.
In order to learn $L$, we can do the following:

1. Construct $\mathbb{G} \is \setof{\wp(\Sigma_\$^n), \subseteq}$, where $n$ is the maximum value required for $L$ (the value of $n$ is given by assumption).
1. Set the conjectured target grammar $G$ to the empty grammar $\emptyset$, which is the lowest point of $\mathbb{G}$.
1. Look at a string $s$ in the data sample.
   If $s \notin L(G)$, find the smallest $G' \subsetneq G$ that contains every $n$-gram in $s$.
   This is the new target grammar.
1. Continue doing this until $G$ no longer changes.

Given sufficient data, this simple procedure will always yield a grammar $G$ that generates $L$.
The grammar does not undergenerate, nor does it overgenerate.
Overgeneration is ruled out by monotonicity: by picking the smallest $G' \supseteq G$ that contains all $n$-grams of $s$, we also pick the smallest possible extension of $L(G)$ that contains $s$.

Note also that this learning algorithm is very fast.
Every change of the target grammar moves the learner upward by at least one level in the lattice.
As each level contains multiple grammars, each update step rules out many grammars at once.
So even though the space of SL-$n$ grammars can be very large, a few steps suffice to home in on the grammar that generates the desired language.

\begin{example}
The lattice in the previous examples contains 16 elements, but only has 5 distinct levels.
Since we already start at level 1, it takes at most 4 updates to find the matching grammar.
\end{example}

\begin{exercise}
Formulate an analogous learning algorithm that instead operates with the lattice of negative SL-$n$ grammars. 
\end{exercise}

## Universals in learning

Albeit simple, the learning algorithm is quite smart.
It builds on the realization that relations don't just hold of linguistic entities, they even extend to entire languages.
Learning becomes much easier once one realizes that the space of possible languages isn't flat but contains hidden structure, too.

However, in order for this to work as desired, the learning algorithm needs to know two parameters in advance: the alphabet, and the value for $n$.
But this is not too much of a problem in practice as both the alphabet and the value for $n$ can be approximated.
For natural language, the alphabet can contain all possible sounds, even if some do not actually occur in the target language.
And since $n$-grams can always be padded out to some higher value, we can also set $n$ to some safe upper bound like $10$.
It is fairly unlikely that any natural language is SL-$n$ only for some $n > 10$.
The downside of picking a large alphabet and high $n$ is that the lattice becomes much bigger, which reduces the speed of the learning algorithm.

\begin{example}
Suppose the target language only contains 40 out of 100 possible sounds.
It is also SL-4 rather than SL-10.
With 100 sounds there are $2^{100^{10}} = 2^{100,000,000,000,000,000,000}$ distinct SL-10 grammars.
This is a huge number: in relation to this number, the amount of atoms in the universe is smaller than a grain of sand relative to Mount Everest.

With 40 sounds and 4-grams, there are "only" $2^{40^4} = 2^{2,560,000}$ distinct grammars.
That's still a mindboggingly large number, but remember that each update rules out a good chunk of those grammars.
So the learning algorithm can still converge fairly quickly on the correct grammar.
\end{example}

Children presumably start out with some safe value for the alphabet $\Sigma$ and $n$ as a genetically encoded universal about the maximum complexity of languages.
As they figure out more about the target language, they prune down $\Sigma$ and $n$ to reduce the size of grammar lattice, which increases the speed of acquisition.
